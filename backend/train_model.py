#!/usr/bin/env python3
"""
Enhanced Training script for Parkinson's Tremor Detection ML models
Supports CLI integration, automated retraining, and feature management

Usage: python train_model.py [model_type] [options]
"""

import argparse
import sys
import os
import json
import glob
from datetime import datetime
import logging
import pandas as pd
import numpy as np

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models import TremorClassifier
from utils import load_sample_data, create_synthetic_data, save_training_data

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ModelManager:
    def __init__(self, models_dir='models'):
        self.models_dir = models_dir
        os.makedirs(models_dir, exist_ok=True)

    def get_latest_model(self):
        """Get the most recent model file"""
        model_files = glob.glob(os.path.join(self.models_dir, 'tremor_model_*.pkl'))
        if not model_files:
            return None

        return max(model_files, key=os.path.getctime)

    def check_for_new_data(self, features_file='data/features/features.csv'):
        """Check if new data has been added since last training"""
        if not os.path.exists(features_file):
            return False

        latest_model = self.get_latest_model()
        if not latest_model:
            return True  # No model exists, need to train

        model_time = os.path.getctime(latest_model)
        data_time = os.path.getctime(features_file)

        return data_time > model_time

    def load_features_from_cli(self, features_file='data/features/features.csv'):
        """Load features generated by CLI trainer"""
        if not os.path.exists(features_file):
            logger.error(f"Features file not found: {features_file}")
            return None, None

        try:
            df = pd.read_csv(features_file)

            # Validate required columns
            required_cols = ['mean', 'rms', 'variance', 'zero_crossings', 'dominant_frequency', 'spectral_entropy', 'label']
            missing_cols = [col for col in required_cols if col not in df.columns]

            if missing_cols:
                logger.error(f"Missing columns in features file: {missing_cols}")
                return None, None

            X = df[required_cols[:-1]].values  # Features
            y = df['label'].values  # Labels

            logger.info(f"Loaded {len(X)} samples from CLI features")
            logger.info(f"Classes: {np.unique(y)}")

            return X, y

        except Exception as e:
            logger.error(f"Error loading features: {e}")
            return None, None

def main():
    parser = argparse.ArgumentParser(description="Enhanced Parkinson's Tremor Detection Model Training")
    parser.add_argument('--model_type', '-m', type=str, default='random_forest',
                       choices=['random_forest', 'svm', 'neural_network'],
                       help='Type of model to train')
    parser.add_argument('--data_source', '-d', type=str, default='cli_features',
                       choices=['sample', 'synthetic', 'cli_features', 'custom'],
                       help='Source of training data')
    parser.add_argument('--custom_data', type=str,
                       help='Path to custom data file for custom source')
    parser.add_argument('--test_size', '-t', type=float, default=0.2,
                       help='Test set size (0-1)')
    parser.add_argument('--output_dir', '-o', type=str, default='models',
                       help='Output directory for model')
    parser.add_argument('--export_format', '-f', type=str, default='pkl',
                       choices=['pkl', 'onnx'],
                       help='Export format for model')
    parser.add_argument('--use_synthetic', action='store_true',
                       help='Use synthetic data instead of real data')
    parser.add_argument('--save_data', action='store_true',
                       help='Save training data for future use')
    parser.add_argument('--auto_retrain', action='store_true',
                       help='Automatically retrain if new data is detected')
    parser.add_argument('--force_retrain', action='store_true',
                       help='Force retrain even if no new data')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Verbose output')

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    logger.info("Starting enhanced model training...")
    logger.info(f"Model type: {args.model_type}")
    logger.info(f"Data source: {args.data_source}")

    model_manager = ModelManager(args.output_dir)

    # Check for auto-retrain
    if args.auto_retrain and not args.force_retrain:
        if not model_manager.check_for_new_data():
            logger.info("No new data detected. Use --force_retrain to train anyway.")
            return 0

    try:
        # Load or generate training data based on source
        if args.data_source == 'cli_features':
            X, y = model_manager.load_features_from_cli()
            if X is None:
                logger.error("Failed to load CLI features. Using synthetic data instead.")
                X, y = create_synthetic_data(1000)

        elif args.data_source == 'synthetic':
            logger.info(f"Generating {args.n_samples if hasattr(args, 'n_samples') else 1000} synthetic samples...")
            X, y = create_synthetic_data(args.n_samples if hasattr(args, 'n_samples') else 1000)

        elif args.data_source == 'custom':
            if not args.custom_data:
                logger.error("Custom data source requires --custom_data path")
                return 1
            logger.info(f"Loading custom data from {args.custom_data}...")
            X, y = load_sample_data(args.custom_data)

        else:  # sample or default
            logger.info("Loading sample data...")
            X, y = load_sample_data(args.data_source)

        if len(X) == 0:
            logger.error("No training data available")
            return 1

        logger.info(f"Loaded {len(X)} training samples with {len(set(y))} classes")
        logger.info(f"Classes: {sorted(set(y))}")

        # Check for class imbalance
        class_counts = pd.Series(y).value_counts()
        logger.info(f"Class distribution: {dict(class_counts)}")

        # Initialize and train classifier
        classifier = TremorClassifier(model_type=args.model_type)

        logger.info("Training model...")
        training_results = classifier.train(
            X, y,
            test_size=args.test_size,
            random_state=42
        )

        # Create output directory if it doesn't exist
        os.makedirs(args.output_dir, exist_ok=True)

        # Save model with enhanced naming
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_filename = f"tremor_model_{args.model_type}_{len(X)}samples_{timestamp}.pkl"
        model_path = os.path.join(args.output_dir, model_filename)

        # For neural networks, handle differently
        if args.model_type == "neural_network":
            model_path = os.path.join(args.output_dir, f"tremor_model_{args.model_type}_{timestamp}")
            classifier.model.save(model_path)
            model_filename = f"tremor_model_{args.model_type}_{timestamp}"
        else:
            import joblib
            joblib.dump(classifier, model_path)

        # Enhanced metadata
        metadata = {
            "model_type": args.model_type,
            "training_date": timestamp,
            "training_results": training_results,
            "data_source": args.data_source,
            "num_samples": len(X),
            "classes": sorted(set(y)),
            "class_distribution": dict(class_counts),
            "model_file": model_filename,
            "feature_names": ['mean', 'rms', 'variance', 'zero_crossings', 'dominant_frequency', 'spectral_entropy'],
            "auto_retrain": args.auto_retrain,
            "force_retrain": args.force_retrain
        }

        metadata_path = os.path.join(args.output_dir, f"metadata_{timestamp}.json")
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        # Save training data if requested
        if args.save_data:
            data_filename = f"training_data_{timestamp}.json"
            data_path = os.path.join("data", data_filename)
            if save_training_data(X, y, data_path):
                logger.info(f"Training data saved to {data_path}")

        # Print comprehensive results
        logger.info("üéâ Training completed successfully!")
        logger.info(f"üìÅ Model saved to: {model_path}")
        logger.info(f"‚è±Ô∏è  Training time: {training_results['training_time']:.2f} seconds")
        logger.info(f"üéØ Accuracy: {training_results['accuracy']:.4f}")
        logger.info(f"üìä Precision: {training_results.get('precision', 0):.4f}")
        logger.info(f"üîÑ Recall: {training_results.get('recall', 0):.4f}")
        logger.info(f"üìà F1 Score: {training_results.get('f1_score', 0):.4f}")

        if 'feature_importance' in training_results and training_results['feature_importance']:
            logger.info("üèÜ Top 5 important features:")
            sorted_features = sorted(
                training_results['feature_importance'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
            for feature, importance in sorted_features:
                logger.info(f"  {feature}: {importance:.4f}")

        # Auto-cleanup old models (keep last 5)
        cleanup_old_models(args.output_dir)

        return 0

    except Exception as e:
        logger.error(f"Training failed: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return 1

def cleanup_old_models(models_dir, keep_count=5):
    """Clean up old model files, keeping only the most recent ones"""
    try:
        model_files = glob.glob(os.path.join(models_dir, 'tremor_model_*.pkl'))
        model_files.sort(key=os.path.getctime, reverse=True)

        if len(model_files) > keep_count:
            for old_file in model_files[keep_count:]:
                os.remove(old_file)
                metadata_file = old_file.replace('.pkl', '.json').replace('tremor_model_', 'metadata_')
                if os.path.exists(metadata_file):
                    os.remove(metadata_file)
                logger.info(f"üóëÔ∏è  Cleaned up old model: {os.path.basename(old_file)}")

    except Exception as e:
        logger.warning(f"Cleanup failed: {e}")

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
